import os
import sys
from dotenv import load_dotenv
from typing import List
from tqdm import tqdm  # å¯¼å…¥ tqdm åº“

# æ ¸å¿ƒä¾èµ–ï¼šLangChainçš„å‘é‡å­˜å‚¨å’ŒEmbeddingåŸºç±»
from langchain_community.vectorstores import Chroma
from langchain_core.embeddings import Embeddings as BaseEmbeddings

# ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·è¦æ±‚ï¼Œä½¿ç”¨åŸç”Ÿçš„ openai å®¢æˆ·ç«¯å’Œ tiktoken
from openai import OpenAI
import tiktoken

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ã€‚
from rag_core.config import CHROMA_DB_PATH, COLLECTION_NAME, EMBEDDING_MODEL_NAME
from rag_core.indexing_utils import get_processed_chunks

# å®šä¹‰é˜¿é‡Œäº‘ DashScope å…¼å®¹æ¨¡å¼çš„ Base URL
DASHSCOPE_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"

# å…³é”®ä¿®å¤ï¼šå®šä¹‰åˆ†æ‰¹å¤„ç†å¤§å°ï¼Œæ ¹æ® DashScope é”™è¯¯ä¿¡æ¯ï¼Œæœ€å¤§å€¼ä¸èƒ½è¶…è¿‡ 10
BATCH_SIZE = 10


class CustomDashScopeEmbeddings(BaseEmbeddings):
    """
    è‡ªå®šä¹‰çš„ Embedding åŒ…è£…å™¨ï¼Œç”¨äºä½¿ç”¨åŸç”Ÿçš„ openai.OpenAI å®¢æˆ·ç«¯
    è°ƒç”¨é˜¿é‡Œäº‘ DashScope çš„å…¼å®¹ APIï¼ŒåŒæ—¶æ»¡è¶³ LangChain Embeddings æ¥å£è¦æ±‚ã€‚
    """

    def __init__(self, model: str, api_key: str, base_url: str):
        self.model = model
        self.client = OpenAI(api_key=api_key, base_url=base_url)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        å®ç° LangChain è¦æ±‚çš„æ–‡æ¡£ Embedding æ–¹æ³•ï¼Œå¹¶å¼ºåˆ¶è¿›è¡Œåˆ†æ‰¹å¤„ç†ã€‚
        """
        all_embeddings: List[List[float]] = []

        # è®¡ç®—æ€»æ‰¹æ¬¡æ•°é‡
        num_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE

        # --- å…³é”®ä¿®å¤ï¼šæ‰‹åŠ¨åˆ†æ‰¹å¤„ç† (Batching)ï¼Œå¹¶ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ ---
        for i in tqdm(range(0, len(texts), BATCH_SIZE),
                      total=num_batches,
                      desc="å‘é‡åŒ–æ‰¹æ¬¡è¿›åº¦"):
            batch = texts[i:i + BATCH_SIZE]

            # è°ƒç”¨åŸç”Ÿçš„ client.embeddings.create API
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=batch
                )
                # ä»å“åº”ä¸­æå– embedding å‘é‡å¹¶æ·»åŠ åˆ°æ€»åˆ—è¡¨
                batch_embeddings = [data.embedding for data in response.data]
                all_embeddings.extend(batch_embeddings)
            except Exception as e:
                # å¦‚æœæŸä¸€æ‰¹æ¬¡å¤±è´¥ï¼Œæ‰“å°é”™è¯¯ï¼Œå¹¶é‡æ–°æŠ›å‡ºä»¥ç»ˆæ­¢æµç¨‹
                tqdm.write(f"âŒ è­¦å‘Šï¼šEmbedding è¿‡ç¨‹ä¸­ï¼Œæ‰¹æ¬¡ {i // BATCH_SIZE} å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {e}")
                raise e

        return all_embeddings

    def embed_query(self, text: str) -> List[float]:
        """
        å®ç° LangChain è¦æ±‚çš„æŸ¥è¯¢ Embedding æ–¹æ³•ã€‚
        """
        # æŸ¥è¯¢é€šå¸¸åªæœ‰ä¸€æ¡ï¼Œä¸éœ€è¦åˆ†æ‰¹
        return self.embed_documents([text])[0]


def build_index():
    """
    æ‰§è¡Œ RAG ç´¢å¼•æ„å»ºçš„ä¸»æµç¨‹ï¼š
    1. åŠ è½½å’Œå¤„ç†æ–‡æ¡£ã€‚
    2. ä¼°ç®— Token æˆæœ¬å¹¶è¯·æ±‚ç”¨æˆ·ç¡®è®¤ã€‚
    3. åˆå§‹åŒ– Embedding æ¨¡å‹ï¼ˆä½¿ç”¨è‡ªå®šä¹‰ Wrapperï¼‰ã€‚
    4. åˆ›å»º Chroma å‘é‡æ•°æ®åº“å¹¶å­˜å‚¨æ•°æ®ã€‚
    """
    print("=" * 60)
    print("ğŸš€ æ‚¬æµ® RAG ç¼–ç¨‹åŠ©æ‰‹ - çŸ¥è¯†ç´¢å¼•æ„å»ºå·¥å…·")
    print("=" * 60)

    # 1. åŠ è½½ç¯å¢ƒå˜é‡ (ç¡®ä¿ DASHSCOPE_API_KEY å·²åœ¨ .env æ–‡ä»¶ä¸­é…ç½®)
    load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))
    api_key = os.getenv("DASHSCOPE_API_KEY")

    if not api_key:
        print("âŒ é”™è¯¯ï¼šDASHSCOPE_API_KEY æœªåœ¨ .env æ–‡ä»¶ä¸­é…ç½®ï¼è¯·æ£€æŸ¥æ‚¨çš„å¯†é’¥ã€‚")
        return

    # 2. è·å–å¤„ç†åçš„æ–‡æ¡£å—
    chunks = get_processed_chunks()
    if not chunks:
        print("ç´¢å¼•æ„å»ºä¸­æ­¢ã€‚")
        return

    # 3. Token ä¼°ç®—å’Œç¡®è®¤
    print("-> æ­£åœ¨ä¼°ç®— Token æ•°é‡...")
    try:
        # ä½¿ç”¨ tiktoken ä¼°ç®— Token
        tokenizer = tiktoken.get_encoding("cl100k_base")

        all_text = [chunk.page_content for chunk in chunks]
        token_counts = [len(tokenizer.encode(text)) for text in all_text]
        total_tokens = sum(token_counts)

        print("-" * 50)
        print(f"ğŸ“ ç´¢å¼•ä»»åŠ¡æ€»ç»“:")
        print(f"   æ€»çŸ¥è¯†å—æ•°é‡: {len(chunks)} ä¸ª")
        print(f"   é¢„è®¡æ€» Token æ•°é‡ (ç”¨äº Embedding): {total_tokens:,} Tokens")
        print(f"   ä½¿ç”¨çš„æ¨¡å‹: {EMBEDDING_MODEL_NAME}")
        print("-" * 50)

        # è¯·æ±‚ç”¨æˆ·ç¡®è®¤
        user_input = input("â“ ç¡®è®¤å¼€å§‹å‘é‡åŒ– (è¿™ä¼šäº§ç”Ÿ API è´¹ç”¨)ï¼Ÿ(è¾“å…¥ 'Y' æˆ– 'N'): ").strip().upper()

        if user_input != 'Y':
            print("ğŸ›‘ ç”¨æˆ·å–æ¶ˆäº†ç´¢å¼•æ„å»ºã€‚")
            return

    except ImportError:
        print("âš ï¸ è­¦å‘Šï¼šæœªå®‰è£… tiktoken åº“ï¼Œæ— æ³•ä¼°ç®— Tokenã€‚ç»§ç»­æ„å»º...")
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šToken ä¼°ç®—æˆ–ç”¨æˆ·ç¡®è®¤å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        return

    # 4. åˆå§‹åŒ– Embedding æ¨¡å‹
    try:
        print(f"-> æ­£åœ¨åˆå§‹åŒ– OpenAI å…¼å®¹ Embedding æ¨¡å‹ (DashScope): {EMBEDDING_MODEL_NAME}...")

        embeddings = CustomDashScopeEmbeddings(
            model=EMBEDDING_MODEL_NAME,
            api_key=api_key,
            base_url=DASHSCOPE_BASE_URL
        )
        print("-> Embedding æ¨¡å‹åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šEmbedding æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ã€‚è¯·æ£€æŸ¥ API Key å’Œä¾èµ–åº“ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        return

    # 5. åˆ›å»º Chroma å‘é‡æ•°æ®åº“å¹¶å­˜å‚¨
    # æ³¨æ„ï¼šChroma.from_documents åœ¨å†…éƒ¨è°ƒç”¨äº† CustomDashScopeEmbeddings.embed_documentsï¼Œ
    # è¿›åº¦æ¡å·²ç»åœ¨ embed_documents ä¸­å®ç°ï¼Œæ— éœ€åœ¨æ­¤å¤„é‡å¤åŒ…è£…ã€‚
    print(f"-> æ­£åœ¨åˆ›å»ºå’Œå¡«å…… Chroma æ•°æ®åº“åˆ°: {CHROMA_DB_PATH}...")
    try:
        Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=CHROMA_DB_PATH,
            collection_name=COLLECTION_NAME
        )

        print("âœ… ç´¢å¼•æ„å»ºæˆåŠŸï¼")
        print(f"å…±è®¡ {len(chunks)} ä¸ªçŸ¥è¯†å—å·²å­˜å‚¨åˆ° ChromaDB çš„ '{COLLECTION_NAME}' é›†åˆä¸­ã€‚")
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šå­˜å‚¨åˆ° ChromaDB å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        # æ‰“å°åŸå§‹é”™è¯¯ä¿¡æ¯ï¼Œå¸®åŠ©è°ƒè¯•
        print(f"åŸå§‹é”™è¯¯è¯¦æƒ…: {e}")


if __name__ == "__main__":
    build_index()